#!/usr/bin/env python3
"""
æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£å¸¸å¹¶æ‰§è¡Œå¿…è¦çš„åˆå§‹åŒ–æ“ä½œ
"""
import os
import sys
import time
import psycopg2
from psycopg2 import sql
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/app')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_db_config():
    """è·å–æ•°æ®åº“é…ç½®"""
    return {
        'host': os.getenv('POSTGRES_HOST', 'postgres'),
        'port': int(os.getenv('POSTGRES_PORT', 5432)),
        'database': os.getenv('POSTGRES_DB', 'chainlit_rag'),
        'user': os.getenv('POSTGRES_USER', 'rag_user'),
        'password': os.getenv('POSTGRES_PASSWORD', 'rag_password'),
    }

def wait_for_database(max_retries=30, retry_interval=2):
    """ç­‰å¾…æ•°æ®åº“å°±ç»ª"""
    db_config = get_db_config()
    
    for attempt in range(max_retries):
        try:
            conn = psycopg2.connect(**db_config)
            conn.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except psycopg2.OperationalError as e:
            if attempt < max_retries - 1:
                logger.info(f"â³ ç­‰å¾…æ•°æ®åº“å°±ç»ª... (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(retry_interval)
            else:
                logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                return False
    return False

def execute_sql_file(filepath):
    """æ‰§è¡ŒSQLæ–‡ä»¶"""
    db_config = get_db_config()
    
    try:
        with psycopg2.connect(**db_config) as conn:
            with conn.cursor() as cursor:
                with open(filepath, 'r', encoding='utf-8') as f:
                    sql_content = f.read()
                
                # æ‰§è¡ŒSQL
                cursor.execute(sql_content)
                conn.commit()
                logger.info(f"âœ… æˆåŠŸæ‰§è¡ŒSQLæ–‡ä»¶: {filepath}")
                return True
                
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡ŒSQLæ–‡ä»¶å¤±è´¥: {e}")
        return False

def check_tables():
    """æ£€æŸ¥è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ"""
    db_config = get_db_config()
    
    expected_tables = [
        'users', 'documents', 'conversations', 
        'chat_messages', 'document_chunks'
    ]
    
    try:
        with psycopg2.connect(**db_config) as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                """)
                
                existing_tables = [row[0] for row in cursor.fetchall()]
                
                missing_tables = set(expected_tables) - set(existing_tables)
                
                if missing_tables:
                    logger.warning(f"âš ï¸  ç¼ºå°‘è¡¨: {missing_tables}")
                    return False
                else:
                    logger.info("âœ… æ‰€æœ‰å¿…éœ€çš„è¡¨éƒ½å·²åˆ›å»º")
                    return True
                    
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥è¡¨æ—¶å‡ºé”™: {e}")
        return False

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = [
        '/app/data',
        '/app/uploads',
        '/app/logs',
        '/app/data/vector_index'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")

def initialize_vector_storage():
    """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
    try:
        vector_index_path = '/app/data/vector_index'
        metadata_path = os.path.join(vector_index_path, 'metadata.json')
        
        # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
        if not os.path.exists(metadata_path):
            import json
            initial_metadata = {
                "created_at": time.time(),
                "version": "1.0",
                "document_count": 0,
                "vector_count": 0
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(initial_metadata, f, indent=2)
            
            logger.info("âœ… å‘é‡å­˜å‚¨å…ƒæ•°æ®åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")

def main():
    """ä¸»åˆå§‹åŒ–å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“åˆå§‹åŒ–...")
    
    # 1. ç­‰å¾…æ•°æ®åº“å°±ç»ª
    if not wait_for_database():
        logger.error("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
        sys.exit(1)
    
    # 2. åˆ›å»ºå¿…è¦ç›®å½•
    create_directories()
    
    # 3. æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    if check_tables():
        logger.info("âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
    else:
        # 4. æ‰§è¡Œåˆå§‹åŒ–SQL
        sql_file = '/app/docker/init.sql'
        if os.path.exists(sql_file):
            if not execute_sql_file(sql_file):
                logger.error("âŒ SQLåˆå§‹åŒ–å¤±è´¥")
                sys.exit(1)
        else:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°SQLåˆå§‹åŒ–æ–‡ä»¶: {sql_file}")
    
    # 5. å†æ¬¡æ£€æŸ¥è¡¨
    if not check_tables():
        logger.error("âŒ æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
        sys.exit(1)
    
    # 6. åˆå§‹åŒ–å‘é‡å­˜å‚¨
    initialize_vector_storage()
    
    logger.info("ğŸ‰ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")

if __name__ == '__main__':
    main()