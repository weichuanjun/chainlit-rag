#!/usr/bin/env python3
"""
æ•´åˆçš„æœåŠ¡å™¨åº”ç”¨
é›†æˆæ‰€æœ‰åç«¯åŠŸèƒ½çš„FlaskæœåŠ¡å™¨
"""
import os
import sys
import json
import uuid
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2 import pool
import redis
import bcrypt
import jwt
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
import aiohttp
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/app')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app, origins=["http://localhost:8000", "http://localhost:80"])

# é…ç½®
class Config:
    # æ•°æ®åº“é…ç½®
    POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'postgres')
    POSTGRES_PORT = int(os.getenv('POSTGRES_PORT', 5432))
    POSTGRES_DB = os.getenv('POSTGRES_DB', 'chainlit_rag')
    POSTGRES_USER = os.getenv('POSTGRES_USER', 'rag_user')
    POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'rag_password')
    
    # Redisé…ç½®
    REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
    REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
    REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', 'redis_password')
    
    # OpenAIé…ç½®
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
    OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')
    OPENAI_EMBEDDING_MODEL = os.getenv('OPENAI_EMBEDDING_MODEL', 'text-embedding-ada-002')
    
    # åº”ç”¨é…ç½®
    JWT_SECRET = os.getenv('JWT_SECRET', 'change-in-production')
    FILE_UPLOAD_PATH = os.getenv('FILE_UPLOAD_PATH', '/app/uploads')
    VECTOR_INDEX_PATH = os.getenv('VECTOR_INDEX_PATH', '/app/data/vector_index')
    
    # æ–‡ä»¶ä¸Šä¼ é…ç½®
    MAX_CONTENT_LENGTH = 50 * 1024 * 1024  # 50MB
    ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

config = Config()

# å…¨å±€æ•°æ®åº“å’ŒRedisè¿æ¥
db_pool = None
redis_client = None

# åˆå§‹åŒ–è¿æ¥
def init_connections():
    """åˆå§‹åŒ–æ•°æ®åº“å’ŒRedisè¿æ¥"""
    global db_pool, redis_client
    
    try:
        # PostgreSQLè¿æ¥æ± 
        db_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=20,
            host=config.POSTGRES_HOST,
            port=config.POSTGRES_PORT,
            database=config.POSTGRES_DB,
            user=config.POSTGRES_USER,
            password=config.POSTGRES_PASSWORD,
            cursor_factory=RealDictCursor
        )
        logger.info("âœ… PostgreSQLè¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")
        
        # Redisè¿æ¥
        redis_client = redis.Redis(
            host=config.REDIS_HOST,
            port=config.REDIS_PORT,
            password=config.REDIS_PASSWORD,
            decode_responses=True
        )
        redis_client.ping()
        logger.info("âœ… Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

# æ•°æ®åº“æ“ä½œè¾…åŠ©å‡½æ•°
def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return db_pool.getconn()

def return_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥"""
    db_pool.putconn(conn)

def execute_query(query, params=None, fetch=True):
    """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢"""
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute(query, params)
            if fetch:
                return cursor.fetchall()
            else:
                conn.commit()
                return cursor.rowcount
    finally:
        return_db_connection(conn)

# è®¤è¯ç›¸å…³å‡½æ•°
def hash_password(password: str) -> str:
    """å¯†ç å“ˆå¸Œ"""
    return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')

def verify_password(password: str, hashed: str) -> bool:
    """éªŒè¯å¯†ç """
    return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))

def generate_jwt_token(user_data: dict) -> str:
    """ç”ŸæˆJWTä»¤ç‰Œ"""
    payload = {
        'user_id': str(user_data['user_id']),
        'email': user_data['email'],
        'username': user_data['username'],
        'role': user_data.get('role', 'user'),
        'iat': datetime.utcnow(),
        'exp': datetime.utcnow() + timedelta(hours=24)
    }
    return jwt.encode(payload, config.JWT_SECRET, algorithm='HS256')

def verify_jwt_token(token: str) -> Optional[dict]:
    """éªŒè¯JWTä»¤ç‰Œ"""
    try:
        payload = jwt.decode(token, config.JWT_SECRET, algorithms=['HS256'])
        return payload
    except jwt.ExpiredSignatureError:
        return None
    except jwt.InvalidTokenError:
        return None

def get_user_from_token() -> Optional[str]:
    """ä»è¯·æ±‚å¤´è·å–ç”¨æˆ·ID"""
    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        return None
    
    token = auth_header[7:]
    payload = verify_jwt_token(token)
    return payload['user_id'] if payload else None

# æ–‡ä»¶å¤„ç†å‡½æ•°
def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in config.ALLOWED_EXTENSIONS

def extract_text_from_file(file_path: str, file_type: str) -> str:
    """ä»æ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        import PyPDF2
        import docx
        import pandas as pd
        import json
        
        logger.info(f"å¼€å§‹æå–æ–‡ä»¶å†…å®¹: {file_path}, ç±»å‹: {file_type}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return f"æ–‡ä»¶ä¸å­˜åœ¨: {os.path.basename(file_path)}"
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        logger.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        if file_size == 0:
            logger.warning(f"æ–‡ä»¶ä¸ºç©º: {file_path}")
            return f"æ–‡ä»¶ä¸ºç©º: {os.path.basename(file_path)}"
        
        if file_type == 'text/plain' or file_path.lower().endswith('.txt'):
            logger.info("å¤„ç†æ–‡æœ¬æ–‡ä»¶")
            # å°è¯•å¤šç§ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        if content.strip():  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                            logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                            return content
                        else:
                            logger.warning(f"ä½¿ç”¨ {encoding} ç¼–ç è¯»å–çš„æ–‡ä»¶å†…å®¹ä¸ºç©º")
                except UnicodeDecodeError as e:
                    logger.debug(f"ä½¿ç”¨ {encoding} ç¼–ç å¤±è´¥: {e}")
                    continue
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨äºŒè¿›åˆ¶è¯»å–
            logger.info("æ‰€æœ‰æ–‡æœ¬ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•äºŒè¿›åˆ¶è¯»å–")
            try:
                with open(file_path, 'rb') as f:
                    content = f.read().decode('latin-1')
                    logger.info(f"äºŒè¿›åˆ¶è¯»å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    return content
            except Exception as e:
                logger.error(f"äºŒè¿›åˆ¶è¯»å–å¤±è´¥: {e}")
                return f"æ–‡ä»¶è¯»å–å¤±è´¥: {os.path.basename(file_path)}"
                
        elif file_type == 'text/markdown' or file_path.lower().endswith('.md'):
            logger.info("å¤„ç†Markdownæ–‡ä»¶")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    logger.info(f"Markdownæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    return content
            except Exception as e:
                logger.error(f"Markdownæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                return f"Markdownæ–‡ä»¶è¯»å–å¤±è´¥: {os.path.basename(file_path)}"
                
        elif file_type == 'application/pdf' or file_path.lower().endswith('.pdf'):
            logger.info("å¤„ç†PDFæ–‡ä»¶")
            try:
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for i, page in enumerate(pdf_reader.pages):
                        page_text = page.extract_text()
                        text += page_text + "\n"
                        logger.debug(f"PDFç¬¬{i+1}é¡µæå–äº† {len(page_text)} å­—ç¬¦")
                    logger.info(f"PDFæ–‡ä»¶å¤„ç†æˆåŠŸï¼Œæ€»å†…å®¹é•¿åº¦: {len(text)}")
                    return text.strip()
            except Exception as e:
                logger.error(f"PDFå¤„ç†å¤±è´¥: {e}")
                return f"PDFæ–‡ä»¶å†…å®¹æå–å¤±è´¥: {os.path.basename(file_path)}"
                
        elif file_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' or file_path.lower().endswith('.docx'):
            logger.info("å¤„ç†DOCXæ–‡ä»¶")
            try:
                doc = docx.Document(file_path)
                text = ""
                for i, paragraph in enumerate(doc.paragraphs):
                    text += paragraph.text + "\n"
                logger.info(f"DOCXæ–‡ä»¶å¤„ç†æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text)}")
                return text.strip()
            except Exception as e:
                logger.error(f"DOCXå¤„ç†å¤±è´¥: {e}")
                return f"DOCXæ–‡ä»¶å†…å®¹æå–å¤±è´¥: {os.path.basename(file_path)}"
                
        elif file_type == 'application/msword' or file_path.lower().endswith('.doc'):
            logger.info("å¤„ç†DOCæ–‡ä»¶")
            # å¯¹äº.docæ–‡ä»¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            return f"DOCæ–‡ä»¶å†…å®¹æå–ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰: {os.path.basename(file_path)}"
            
        elif file_type == 'text/csv' or file_path.lower().endswith('.csv'):
            logger.info("å¤„ç†CSVæ–‡ä»¶")
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                content = df.to_string(index=False)
                logger.info(f"CSVæ–‡ä»¶å¤„ç†æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                return content
            except Exception as e:
                logger.error(f"CSVå¤„ç†å¤±è´¥: {e}")
                return f"CSVæ–‡ä»¶å†…å®¹æå–å¤±è´¥: {os.path.basename(file_path)}"
                
        elif file_type == 'application/json' or file_path.lower().endswith('.json'):
            logger.info("å¤„ç†JSONæ–‡ä»¶")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    content = json.dumps(data, ensure_ascii=False, indent=2)
                    logger.info(f"JSONæ–‡ä»¶å¤„ç†æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    return content
            except Exception as e:
                logger.error(f"JSONå¤„ç†å¤±è´¥: {e}")
                return f"JSONæ–‡ä»¶å†…å®¹æå–å¤±è´¥: {os.path.basename(file_path)}"
        else:
            logger.info(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–: {file_type}")
            # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    logger.info(f"ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    return content
            except Exception as e:
                logger.error(f"ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                return f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type} (æ–‡ä»¶: {os.path.basename(file_path)})"
                
    except Exception as e:
        logger.error(f"æ–‡æœ¬æå–å¤±è´¥: {e}")
        return f"æ–‡ä»¶å†…å®¹æå–å¤±è´¥: {os.path.basename(file_path)}"

def split_text_into_chunks(text: str, max_chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
    if not text or len(text) <= max_chunk_size:
        return [text] if text else []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦æˆ–ç©ºæ ¼å¤„åˆ†å‰²
        if end < len(text):
            # å¯»æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹
            split_points = []
            
            # å¥å·åˆ†å‰²
            period_pos = text.rfind('.', start, end)
            if period_pos > start + max_chunk_size // 2:  # ç¡®ä¿ä¸ä¼šå¤ªçŸ­
                split_points.append(period_pos + 1)
            
            # æ¢è¡Œç¬¦åˆ†å‰²
            newline_pos = text.rfind('\n', start, end)
            if newline_pos > start + max_chunk_size // 2:
                split_points.append(newline_pos + 1)
            
            # ç©ºæ ¼åˆ†å‰²
            space_pos = text.rfind(' ', start, end)
            if space_pos > start + max_chunk_size // 2:
                split_points.append(space_pos + 1)
            
            # é€‰æ‹©æœ€æ¥è¿‘endçš„åˆ†å‰²ç‚¹
            if split_points:
                end = max(split_points)
        
        chunk = text[start:end].strip()
        if chunk:  # åªæ·»åŠ éç©ºå—
            chunks.append(chunk)
        
        # è®¡ç®—ä¸‹ä¸€å—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
        start = max(start + 1, end - overlap)
    
    return chunks

# å‘é‡å¤„ç†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
def generate_embeddings(texts: List[str]) -> List[List[float]]:
    """ç”Ÿæˆå‘é‡åµŒå…¥"""
    if not config.OPENAI_API_KEY:
        logger.warning("OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
        return [[0.1] * 1536 for _ in texts]
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=config.OPENAI_API_KEY)
        
        embeddings = []
        for text in texts:
            if not text.strip():
                # ç©ºæ–‡æœ¬ä½¿ç”¨é›¶å‘é‡
                embeddings.append([0.0] * 1536)
                continue
                
            response = client.embeddings.create(
                input=text,
                model=config.OPENAI_EMBEDDING_MODEL
            )
            embeddings.append(response.data[0].embedding)
        
        logger.info(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡åµŒå…¥")
        return embeddings
        
    except Exception as e:
        logger.error(f"å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
        # å›é€€åˆ°æ¨¡æ‹Ÿå‘é‡
        return [[0.1] * 1536 for _ in texts]

def save_vectors_to_faiss(vectors: List[List[float]], document_id: str):
    """ä¿å­˜å‘é‡åˆ°FAISS"""
    metadata_path = os.path.join(config.VECTOR_INDEX_PATH, 'metadata.json')
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {}
        
        # ç¡®ä¿ documents é”®å­˜åœ¨
        if "documents" not in metadata:
            metadata["documents"] = {}
        
        # ç¡®ä¿ vectors é”®å­˜åœ¨
        if "vectors" not in metadata:
            metadata["vectors"] = {}
        
        # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
        metadata["documents"][document_id] = {
            "vector_count": len(vectors),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # ä¿å­˜å‘é‡æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä½¿ç”¨FAISSç´¢å¼•ï¼‰
        for i, vector in enumerate(vectors):
            vector_id = f"{document_id}_{i}"
            metadata["vectors"][vector_id] = {
                "document_id": document_id,
                "chunk_id": i,
                "vector": vector,
                "created_at": datetime.utcnow().isoformat()
            }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        logger.info(f"å‘é‡ä¿å­˜æˆåŠŸ: document_id={document_id}, vector_count={len(vectors)}")
            
    except Exception as e:
        logger.error(f"å‘é‡ä¿å­˜å¤±è´¥: {e}")

def search_similar_documents(query_text: str, user_id: str, top_k: int = 3) -> List[Dict]:
    """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
    try:
        if not config.OPENAI_API_KEY:
            logger.warning("OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨ç®€å•æœç´¢")
            return _simple_text_search(query_text, user_id, top_k)
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embeddings = generate_embeddings([query_text])
        if not query_embeddings:
            return _simple_text_search(query_text, user_id, top_k)
        
        query_vector = query_embeddings[0]
        
        # ä»å…ƒæ•°æ®æ–‡ä»¶åŠ è½½å‘é‡æ•°æ®
        metadata_path = os.path.join(config.VECTOR_INDEX_PATH, 'metadata.json')
        if not os.path.exists(metadata_path):
            return _simple_text_search(query_text, user_id, top_k)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for vector_id, vector_data in metadata.get("vectors", {}).items():
            if vector_data.get("document_id"):
                # è·å–æ–‡æ¡£ä¿¡æ¯
                doc_info = execute_query("""
                    SELECT document_id, filename, original_filename, content_text, user_id
                    FROM documents 
                    WHERE document_id = %s AND user_id = %s AND status = 'processed'
                """, (vector_data["document_id"], user_id))
                
                if doc_info:
                    doc = doc_info[0]
                    stored_vector = vector_data.get("vector", [])
                    
                    if len(stored_vector) == len(query_vector):
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        import numpy as np
                        query_np = np.array(query_vector)
                        stored_np = np.array(stored_vector)
                        
                        # å½’ä¸€åŒ–å‘é‡
                        query_norm = np.linalg.norm(query_np)
                        stored_norm = np.linalg.norm(stored_np)
                        
                        if query_norm > 0 and stored_norm > 0:
                            similarity = np.dot(query_np, stored_np) / (query_norm * stored_norm)
                            
                            # æå–å¯¹åº”çš„æ–‡æœ¬å—
                            content_text = doc['content_text']
                            chunk_size = 1000
                            chunk_start = vector_data.get("chunk_id", 0) * 800
                            chunk_end = min(chunk_start + chunk_size, len(content_text))
                            chunk_text = content_text[chunk_start:chunk_end]
                            
                            similarities.append({
                                'document_id': str(doc['document_id']),
                                'filename': doc['original_filename'] or doc['filename'],
                                'content': chunk_text,
                                'similarity_score': similarity,
                                'chunk_id': vector_data.get("chunk_id", 0)
                            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # å»é‡å¹¶è¿”å›top_kä¸ªç»“æœ
        seen_docs = set()
        unique_results = []
        for item in similarities:
            doc_key = f"{item['document_id']}_{item['chunk_id']}"
            if doc_key not in seen_docs:
                seen_docs.add(doc_key)
                unique_results.append(item)
                if len(unique_results) >= top_k:
                    break
        
        logger.info(f"å‘é‡æœç´¢æ‰¾åˆ° {len(unique_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        return unique_results
        
    except Exception as e:
        logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return _simple_text_search(query_text, user_id, top_k)

def _simple_text_search(query_text: str, user_id: str, top_k: int = 3) -> List[Dict]:
    """ç®€å•æ–‡æœ¬æœç´¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    try:
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        documents = execute_query("""
            SELECT document_id, filename, original_filename, content_text
            FROM documents 
            WHERE user_id = %s AND status = 'processed' AND content_text IS NOT NULL
            ORDER BY created_at DESC
            LIMIT 10
        """, (user_id,))
        
        results = []
        for doc in documents:
            content_text = doc['content_text']
            if content_text and len(content_text) > 10:
                # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
                query_words = set(query_text.lower().split())
                content_words = set(content_text.lower().split())
                
                if query_words & content_words:  # æœ‰å…±åŒè¯æ±‡
                    intersection = len(query_words & content_words)
                    union = len(query_words | content_words)
                    similarity = intersection / union if union > 0 else 0
                    
                    content_preview = content_text[:200] + "..." if len(content_text) > 200 else content_text
                    
                    results.append({
                        'document_id': str(doc['document_id']),
                        'filename': doc['original_filename'] or doc['filename'],
                        'content': content_preview,
                        'similarity_score': similarity
                    })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        return results[:top_k]
        
    except Exception as e:
        logger.error(f"ç®€å•æœç´¢å¤±è´¥: {e}")
        return []

# =====================================
# API è·¯ç”±
# =====================================

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.utcnow().isoformat(),
        'services': {
            'database': 'connected',
            'redis': 'connected',
            'openai': 'configured' if config.OPENAI_API_KEY else 'not_configured'
        }
    })

# è®¤è¯API
@app.route('/auth/register', methods=['POST'])
def register():
    """ç”¨æˆ·æ³¨å†Œ"""
    try:
        data = request.get_json()
        email = data.get('email')
        username = data.get('username')
        password = data.get('password')
        full_name = data.get('full_name')
        
        if not all([email, username, password]):
            return jsonify({'error': 'é‚®ç®±ã€ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º'}), 400
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å­˜åœ¨
        existing_user = execute_query(
            "SELECT user_id FROM users WHERE email = %s",
            (email,)
        )
        
        if existing_user:
            return jsonify({'error': 'é‚®ç®±å·²è¢«æ³¨å†Œ'}), 409
        
        # åˆ›å»ºç”¨æˆ·
        password_hash = hash_password(password)
        user_id = str(uuid.uuid4())
        
        execute_query("""
            INSERT INTO users (user_id, email, username, password_hash, full_name)
            VALUES (%s, %s, %s, %s, %s)
        """, (user_id, email, username, password_hash, full_name), fetch=False)
        
        # ç”Ÿæˆä»¤ç‰Œ
        user_data = {
            'user_id': user_id,
            'email': email,
            'username': username,
            'role': 'user'
        }
        access_token = generate_jwt_token(user_data)
        
        return jsonify({
            'message': 'æ³¨å†ŒæˆåŠŸ',
            'access_token': access_token,
            'token_type': 'Bearer',
            'expires_in': 24 * 3600,
            'user': user_data
        }), 201
        
    except Exception as e:
        logger.error(f"æ³¨å†Œå¤±è´¥: {e}")
        return jsonify({'error': 'æ³¨å†Œå¤±è´¥'}), 500

@app.route('/auth/login', methods=['POST'])
def login():
    """ç”¨æˆ·ç™»å½•"""
    try:
        data = request.get_json()
        email = data.get('email')
        password = data.get('password')
        
        logger.info(f"ç™»å½•è¯·æ±‚: email={email}")
        
        if not email or not password:
            logger.warning("ç™»å½•å¤±è´¥: é‚®ç®±æˆ–å¯†ç ä¸ºç©º")
            return jsonify({'error': 'é‚®ç®±å’Œå¯†ç ä¸èƒ½ä¸ºç©º'}), 400
        
        # æŸ¥æ‰¾ç”¨æˆ·
        users = execute_query(
            "SELECT * FROM users WHERE email = %s AND is_active = true",
            (email,)
        )
        
        logger.info(f"æ•°æ®åº“æŸ¥è¯¢ç»“æœ: æ‰¾åˆ° {len(users) if users else 0} ä¸ªç”¨æˆ·")
        
        if not users:
            logger.warning(f"ç™»å½•å¤±è´¥: ç”¨æˆ·ä¸å­˜åœ¨ email={email}")
            return jsonify({'error': 'é‚®ç®±æˆ–å¯†ç é”™è¯¯'}), 401
        
        user = users[0]
        password_verified = verify_password(password, user['password_hash'])
        logger.info(f"å¯†ç éªŒè¯ç»“æœ: {password_verified}")
        
        if not password_verified:
            logger.warning(f"ç™»å½•å¤±è´¥: å¯†ç é”™è¯¯ email={email}")
            return jsonify({'error': 'é‚®ç®±æˆ–å¯†ç é”™è¯¯'}), 401
        
        # æ›´æ–°æœ€åç™»å½•æ—¶é—´
        execute_query(
            "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE user_id = %s",
            (user['user_id'],), fetch=False
        )
        
        # ç”Ÿæˆä»¤ç‰Œ
        access_token = generate_jwt_token(user)
        
        return jsonify({
            'access_token': access_token,
            'token_type': 'Bearer',
            'expires_in': 24 * 3600,
            'user': {
                'user_id': str(user['user_id']),
                'email': user['email'],
                'username': user['username'],
                'full_name': user['full_name'],
                'role': user['role']
            }
        })
        
    except Exception as e:
        logger.error(f"ç™»å½•å¤±è´¥: {e}")
        return jsonify({'error': 'ç™»å½•å¤±è´¥'}), 500

@app.route('/auth/verify', methods=['POST'])
def verify_token():
    """éªŒè¯ä»¤ç‰Œ"""
    user_id = get_user_from_token()
    if not user_id:
        return jsonify({'error': 'æ— æ•ˆä»¤ç‰Œ'}), 401
    
    try:
        users = execute_query(
            "SELECT * FROM users WHERE user_id = %s AND is_active = true",
            (user_id,)
        )
        
        if not users:
            return jsonify({'error': 'ç”¨æˆ·ä¸å­˜åœ¨'}), 401
        
        user = users[0]
        return jsonify({
            'valid': True,
            'user': {
                'user_id': str(user['user_id']),
                'email': user['email'],
                'username': user['username'],
                'full_name': user['full_name'],
                'role': user['role']
            }
        })
        
    except Exception as e:
        logger.error(f"ä»¤ç‰ŒéªŒè¯å¤±è´¥: {e}")
        return jsonify({'error': 'ä»¤ç‰ŒéªŒè¯å¤±è´¥'}), 401

# æ–‡æ¡£API
@app.route('/documents/upload', methods=['POST'])
def upload_document():
    """æ–‡æ¡£ä¸Šä¼ """
    user_id = get_user_from_token()
    if not user_id:
        return jsonify({'error': 'æœªæˆæƒè®¿é—®'}), 401
    
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ–‡ä»¶åä¸èƒ½ä¸ºç©º'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'}), 400
        
        # ä¿å­˜æ–‡ä»¶
        filename = secure_filename(file.filename)
        document_id = str(uuid.uuid4())
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        os.makedirs(config.FILE_UPLOAD_PATH, exist_ok=True)
        
        file_path = os.path.join(config.FILE_UPLOAD_PATH, f"{document_id}_{filename}")
        
        logger.info(f"ä¿å­˜æ–‡ä»¶: {file_path}")
        logger.info(f"åŸå§‹æ–‡ä»¶å: {file.filename}")
        logger.info(f"å®‰å…¨æ–‡ä»¶å: {filename}")
        logger.info(f"æ–‡ä»¶ç±»å‹: {file.content_type}")
        
        file.save(file_path)
        file_size = os.path.getsize(file_path)
        
        logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œå¤§å°: {file_size} bytes")
        
        # æå–æ–‡æœ¬
        content_text = extract_text_from_file(file_path, file.content_type)
        
        logger.info(f"æ–‡æœ¬æå–ç»“æœé•¿åº¦: {len(content_text) if content_text else 0}")
        
        # ä¿å­˜æ–‡æ¡£è®°å½•
        execute_query("""
            INSERT INTO documents (
                document_id, user_id, filename, original_filename, 
                file_type, file_size, file_path, status, content_text
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            document_id, user_id, filename, file.filename,
            file.content_type, file_size, file_path, 'processed', content_text
        ), fetch=False)
        
        logger.info(f"æ–‡æ¡£è®°å½•ä¿å­˜æˆåŠŸ: {document_id}")
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        if content_text and not content_text.startswith("æ–‡ä»¶å†…å®¹æå–å¤±è´¥") and not content_text.startswith("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"):
            logger.info("å¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥...")
            # æ™ºèƒ½åˆ†å—ç­–ç•¥
            chunks = split_text_into_chunks(content_text)
            logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
            
            vectors = generate_embeddings(chunks)
            logger.info(f"å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(vectors)} ä¸ªå‘é‡")
            
            save_vectors_to_faiss(vectors, document_id)
            
            # æ›´æ–°å—æ•°å’Œå‘é‡æ•°
            execute_query("""
                UPDATE documents 
                SET chunk_count = %s, vector_count = %s, processed_at = CURRENT_TIMESTAMP
                WHERE document_id = %s
            """, (len(chunks), len(vectors), document_id), fetch=False)
            
            logger.info(f"å‘é‡ä¿¡æ¯æ›´æ–°å®Œæˆ")
        else:
            logger.warning(f"è·³è¿‡å‘é‡ç”Ÿæˆï¼ŒåŸå› : {content_text[:100] if content_text else 'å†…å®¹ä¸ºç©º'}")
        
        return jsonify({
            'message': 'æ–‡æ¡£ä¸Šä¼ æˆåŠŸ',
            'document_id': document_id,
            'filename': file.filename,  # è¿”å›åŸå§‹æ–‡ä»¶å
            'status': 'processed'
        })
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        return jsonify({'error': 'æ–‡æ¡£ä¸Šä¼ å¤±è´¥'}), 500

@app.route('/documents', methods=['GET'])
def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    user_id = get_user_from_token()
    if not user_id:
        return jsonify({'error': 'æœªæˆæƒè®¿é—®'}), 401
    
    try:
        documents = execute_query("""
            SELECT document_id, filename, original_filename, file_type, 
                   file_size, status, created_at, chunk_count, vector_count, tags
            FROM documents 
            WHERE user_id = %s 
            ORDER BY created_at DESC
        """, (user_id,))
        
        # è½¬æ¢UUIDä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶ç¡®ä¿æ˜¾ç¤ºåŸæ–‡ä»¶å
        for doc in documents:
            doc['document_id'] = str(doc['document_id'])
            doc['created_at'] = doc['created_at'].isoformat()
            # ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç³»ç»Ÿæ–‡ä»¶å
            doc['display_name'] = doc['original_filename'] if doc['original_filename'] else doc['filename']
        
        return jsonify({
            'documents': documents,
            'total_count': len(documents),
            'has_more': False
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': 'è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥'}), 500

# èŠå¤©API
@app.route('/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    user_id = get_user_from_token()
    if not user_id:
        return jsonify({'error': 'æœªæˆæƒè®¿é—®'}), 401
    
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        conversation_id = data.get('conversation_id')
        agent_workflow = data.get('agent_workflow', 'default_rag')
        
        if not message:
            return jsonify({'error': 'æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        # åˆ›å»ºæˆ–è·å–å¯¹è¯
        if not conversation_id:
            conversation_id = str(uuid.uuid4())
            execute_query("""
                INSERT INTO conversations (conversation_id, user_id, agent_workflow)
                VALUES (%s, %s, %s)
            """, (conversation_id, user_id, agent_workflow), fetch=False)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_message_id = str(uuid.uuid4())
        execute_query("""
            INSERT INTO chat_messages (
                message_id, conversation_id, user_id, role, content, agent_workflow
            ) VALUES (%s, %s, %s, %s, %s, %s)
        """, (user_message_id, conversation_id, user_id, 'user', message, agent_workflow), fetch=False)
        
        # å®ç°çœŸæ­£çš„RAGåŠŸèƒ½
        # 1. æœç´¢ç›¸å…³æ–‡æ¡£
        relevant_documents = []
        goto_simple_answer = False
        
        if config.OPENAI_API_KEY:
            try:
                # ä½¿ç”¨å‘é‡æœç´¢
                relevant_documents = search_similar_documents(message, user_id)
                
                # å¦‚æœå‘é‡æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æœç´¢
                if not relevant_documents:
                    logger.warning("å‘é‡æœç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå›é€€åˆ°ç®€å•æœç´¢")
                    relevant_documents = _simple_text_search(message, user_id)
                
                # å¦‚æœç®€å•æœç´¢ä¹Ÿå¤±è´¥ï¼Œåˆ™æ²¡æœ‰ç›¸å…³æ–‡æ¡£
                if not relevant_documents:
                    logger.warning("ç®€å•æœç´¢ä¹Ÿæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤å›ç­”")
                    goto_simple_answer = True
                    
            except Exception as e:
                logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
                # å›é€€åˆ°ç®€å•æœç´¢
                relevant_documents = _simple_text_search(message, user_id)
                if not relevant_documents:
                    goto_simple_answer = True
        
        # 2. ç”Ÿæˆå›ç­”
        if goto_simple_answer or not relevant_documents:
            # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ—¶çš„å›ç­”
            try:
                from openai import OpenAI
                client = OpenAI(api_key=config.OPENAI_API_KEY)
                
                prompt = f"""ç”¨æˆ·é—®é¢˜: {message}

ç”¨æˆ·æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œè¯·å‹å¥½åœ°æç¤ºç”¨æˆ·ä¸Šä¼ æ–‡æ¡£æˆ–æä¾›ä¸€èˆ¬æ€§çš„å¸®åŠ©ã€‚"""
                
                response = client.chat.completions.create(
                    model=config.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œå½“ç”¨æˆ·æ²¡æœ‰ä¸Šä¼ ç›¸å…³æ–‡æ¡£æ—¶ï¼Œè¯·æç¤ºä»–ä»¬ä¸Šä¼ æ–‡æ¡£æˆ–æä¾›å¸®åŠ©ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=500,
                    temperature=0.7
                )
                
                response_content = response.choices[0].message.content
                
            except Exception as e:
                logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
                # å›é€€åˆ°ç®€å•å›ç­”
                response_content = f"""æ‚¨å¥½ï¼æˆ‘æ”¶åˆ°äº†æ‚¨çš„é—®é¢˜ï¼šã€Œ{message}ã€

ç›®å‰æˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£ä¿¡æ¯ã€‚æ‚¨å¯ä»¥ï¼š
1. ä¸Šä¼ ä¸€äº›æ–‡æ¡£åˆ°çŸ¥è¯†åº“
2. é‡æ–°æé—®
3. æˆ–è€…ç›´æ¥å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆ

æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ï¼"""
        else:
            # æœ‰ç›¸å…³æ–‡æ¡£æ—¶çš„å›ç­”
            # æ„å»ºæç¤ºè¯
            doc_summary = "\n".join([f"æ–‡æ¡£: {doc['filename']}\nå†…å®¹: {doc['content']}\n" for doc in relevant_documents])
            
            # è°ƒç”¨OpenAI APIç”Ÿæˆå›ç­”
            try:
                from openai import OpenAI
                client = OpenAI(api_key=config.OPENAI_API_KEY)
                
                prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: {message}

ç›¸å…³æ–‡æ¡£:
{doc_summary}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜éœ€è¦æ›´å¤šä¿¡æ¯ã€‚"""
                
                logger.info(f"å‘é€åˆ°OpenAIçš„æç¤ºè¯: {prompt[:200]}...")
                
                response = client.chat.completions.create(
                    model=config.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åŠ©æ‰‹ï¼ŒåŸºäºç”¨æˆ·æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000,
                    temperature=0.7
                )
                
                response_content = response.choices[0].message.content
                logger.info(f"OpenAIè¿”å›çš„å›ç­”: {response_content[:100]}...")
                
            except Exception as e:
                logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
                # å›é€€åˆ°ç®€å•å›ç­”
                doc_summary = "\n".join([f"- {doc['filename']}: {doc['content']}" for doc in relevant_documents])
                response_content = f"""åŸºäºæ‚¨çš„é—®é¢˜ã€Œ{message}ã€ï¼Œæˆ‘ä¸ºæ‚¨æœç´¢äº†ç›¸å…³æ–‡æ¡£ï¼š

{doc_summary}

æ ¹æ®è¿™äº›æ–‡æ¡£å†…å®¹ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹å›ç­”ï¼š

è¿™æ˜¯ä¸€ä¸ªåŸºäºæ‚¨ä¸Šä¼ æ–‡æ¡£çš„æ™ºèƒ½å›ç­”ã€‚å¦‚æœæ‚¨éœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„é—®é¢˜ã€‚"""
        
        # ä¿å­˜AIå›å¤
        ai_message_id = str(uuid.uuid4())
        execute_query("""
            INSERT INTO chat_messages (
                message_id, conversation_id, user_id, role, content, agent_workflow
            ) VALUES (%s, %s, %s, %s, %s, %s)
        """, (ai_message_id, conversation_id, user_id, 'assistant', response_content, agent_workflow), fetch=False)
        
        # æ›´æ–°å¯¹è¯æ¶ˆæ¯æ•°
        execute_query("""
            UPDATE conversations 
            SET message_count = message_count + 2, updated_at = CURRENT_TIMESTAMP
            WHERE conversation_id = %s
        """, (conversation_id,), fetch=False)
        
        return jsonify({
            'message_id': ai_message_id,
            'conversation_id': conversation_id,
            'content': response_content,
            'used_documents': relevant_documents,
            'reasoning_steps': [
                {'step': 'preprocessing', 'description': 'å¤„ç†ç”¨æˆ·æŸ¥è¯¢'},
                {'step': 'retrieval', 'description': f'æœç´¢åˆ° {len(relevant_documents)} ä¸ªç›¸å…³æ–‡æ¡£'},
                {'step': 'generation', 'description': 'åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”'}
            ],
            'agent_workflow': agent_workflow
        })
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': 'èŠå¤©å¤„ç†å¤±è´¥'}), 500

@app.route('/chat/conversations', methods=['GET'])
def get_conversations():
    """è·å–å¯¹è¯åˆ—è¡¨"""
    user_id = get_user_from_token()
    if not user_id:
        return jsonify({'error': 'æœªæˆæƒè®¿é—®'}), 401
    
    try:
        conversations = execute_query("""
            SELECT c.conversation_id, c.title, c.agent_workflow, c.message_count,
                   c.created_at, c.updated_at,
                   (SELECT content FROM chat_messages 
                    WHERE conversation_id = c.conversation_id AND role = 'user'
                    ORDER BY created_at DESC LIMIT 1) as preview
            FROM conversations c
            WHERE c.user_id = %s
            ORDER BY c.updated_at DESC
        """, (user_id,))
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        for conv in conversations:
            conv['conversation_id'] = str(conv['conversation_id'])
            conv['created_at'] = conv['created_at'].isoformat()
            conv['updated_at'] = conv['updated_at'].isoformat()
            conv['last_message_at'] = conv['updated_at']
            conv['preview'] = conv['preview'][:100] if conv['preview'] else ''
        
        return jsonify({
            'conversations': conversations,
            'total_count': len(conversations)
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': 'è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥'}), 500

# é”™è¯¯å¤„ç†
@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'æœªæ‰¾åˆ°è¯·æ±‚çš„èµ„æº'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ å¯åŠ¨æ•´åˆæœåŠ¡å™¨...")
    
    # åˆå§‹åŒ–è¿æ¥
    init_connections()
    
    # åˆ›å»ºä¸Šä¼ ç›®å½•
    os.makedirs(config.FILE_UPLOAD_PATH, exist_ok=True)
    os.makedirs(config.VECTOR_INDEX_PATH, exist_ok=True)
    
    logger.info("âœ… æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("ğŸ“¡ APIæœåŠ¡å™¨å¯åŠ¨åœ¨: http://0.0.0.0:5000")
    
    app.run(host='0.0.0.0', port=5000, debug=False)